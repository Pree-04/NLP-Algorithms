{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d884f96",
   "metadata": {},
   "source": [
    "# NAME: PREETHA S\n",
    "# USN: 21BTRCL078\n",
    "# NLP Sem 6\n",
    "# Assignment 1\n",
    "# Viterbi Decoding for HMM- Baum Welch Algorithm- Maximum Entropy Model in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d91c9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hmmlearn\n",
      "  Downloading hmmlearn-0.3.0-cp39-cp39-win_amd64.whl (123 kB)\n",
      "     ------------------------------------ 123.8/123.8 kB 154.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from hmmlearn) (1.0.2)\n",
      "Requirement already satisfied: scipy>=0.19 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from hmmlearn) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.10 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from hmmlearn) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\preet\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.1.0)\n",
      "Installing collected packages: hmmlearn\n",
      "Successfully installed hmmlearn-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "064ed2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from hmmlearn import hmm\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.corpus import brown\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "159c0db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\preet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7106c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a sample dataset\n",
    "data = brown.tagged_sents(categories='news')[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93a7338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "train_data = data[:800]\n",
    "test_data = data[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f65040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert tagged sentences to (word, tag) pairs\n",
    "def get_word_tag_pairs(tagged_sentences):\n",
    "    return [(word.lower(), tag) for sentence in tagged_sentences for word, tag in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee8ccb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_tag_pairs = get_word_tag_pairs(train_data)\n",
    "test_word_tag_pairs = get_word_tag_pairs(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79cfe480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction for MaxEnt\n",
    "def word_features(word):\n",
    "    return {'suffix1': word[-1:], 'suffix2': word[-2:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80c3246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -4.95583        0.010\n",
      "             2          -1.45244        0.644\n",
      "             3          -1.12421        0.652\n",
      "             4          -0.98075        0.653\n",
      "             5          -0.90083        0.653\n",
      "             6          -0.85028        0.654\n",
      "             7          -0.81553        0.654\n",
      "             8          -0.79021        0.654\n",
      "             9          -0.77094        0.654\n",
      "         Final          -0.75576        0.655\n"
     ]
    }
   ],
   "source": [
    "# Training MaxEnt model\n",
    "train_features = [(word_features(word), tag) for word, tag in train_word_tag_pairs]\n",
    "maxent_classifier = MaxentClassifier.train(train_features, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2044f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM parameters initialization\n",
    "states = list(set(tag for word, tag in train_word_tag_pairs))\n",
    "symbols = list(set(word for word, tag in train_word_tag_pairs))\n",
    "initial_prob = np.ones(len(states)) / len(states)\n",
    "transition_prob = np.ones((len(states), len(states))) / len(states)\n",
    "emission_prob = np.ones((len(states), len(symbols))) / len(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c859c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
